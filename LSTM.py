# -*- coding: utf-8 -*-
"""Untitled6.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1chKf-xgkIkA7qKp-xsKRQrGT3qdaw5Xt
"""

#Loading the dataset
import pandas as pd
df_data=pd.DataFrame()
x1=[]
x2=[]
x3=[]
y=[]
for i in range(1,1001):
    x1=x1+[i]
    x2=x2+[i+1]
    x3=x3+[i+2]
    y=y+[i+3]
df_data['x1']=x1
df_data['x2']=x2
df_data['x3']=x3
df_data['y']=y
df_data.to_csv("data.csv")

import tensorflow
import numpy as np
#tensorflow.Version
from keras.models import Sequential
from keras.layers import Dense                         #Dense layer is the regular deeply connected neural network layer.
from keras.layers import LSTM, TimeDistributed

#The Alogorithm Used is:::::
#In sequence prediction challenges, Long Short Term Memory (LSTM) networks are a type of Recurrent Neural Network that can learn order dependence. 
#The output of the previous step is used as input in the current step in RNN.

import numpy as np
x=np.array(df_data.iloc[:,0:3])
y=np.array(df_data.iloc[:,3])
x,y
x=x.reshape(1000,3,1)
x.shape
y=y.reshape(1000,1,1)
y.shape
model=Sequential()
model.add(LSTM(input_shape=(3,1), 
                    units=3,
                    activation='relu'))

#The Activation Function Used here is::::
#The Rectified Linear Unit is the most commonly used activation function in deep learning models. 
#The function returns 0 if it receives any negative input, but for any positive value x it returns that value back.


model.add(Dense(activation='linear', units=1))
model.compile(loss = 'mse', optimizer = 'rmsprop')
np.random.seed(7)
model.fit(x, y, epochs = 500, batch_size = 8)
test = np.array([2003,2004,2005])
test.shape
test= test.reshape(1,3,1)
model.predict(test)